---
title: "Ordinary Least Squares, continued"
subtitle: "EDS 222"
author: "Tamma Carleton"
#institute: ""
date: "Fall 2023"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r Setup, include = F}
options(htmltools.dir.version = FALSE)
library(pacman)
p_load(leaflet, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr,pagedown,cowplot,latex2exp,parallel)
# Define pink color
red_pink <- "#e64173"
# Notes directory
dir_slides <- "~/Dropbox/Teaching/UCSB/EDS_222/EDS222_code/EDS-222-stats/slides/04-ols-contd/"
# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  # dpi = 300,
  # cache = T,
  warning = F,
  message = F,
  dev = "svg"
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```


# Announcements/check-in

- Assignment #1: Grades posted
  + Please ensure your `.html` file is compiled and pushed to GitHub
  + Please do not push data to GitHub (generally a good rule to follow)
  + Sandy to go over some areas of confusion

- Assignment #2: Due 10/20, 5pm

--

- Final project guidelines posted (under "Important Links" on homepage and on Resources tab)

--

- Practice midterm questions posted sometime this week

--

- Reiteration of COVID/illness policy

---
name: Overview

# Today

#### Notes on OLS
- Outliers, missing data

--

#### Measures of model fit
- Coefficient of variation $R^2$

--

#### Categorical variables
- In .mono[R], interpretation

--

#### Multiple linear regression
- Adding independent variables, interpretation of results

---

layout: false
class: clear, middle, inverse
# Notes on OLS

---
# Outliers

Because OLS minimizes the sum of the **squared** errors, outliers can play a large role in our estimates.

**Common responses**

- Remove the outliers from the dataset

- Replace outliers with the 99<sup>th</sup> percentile of their variable (*winsorize*)

- Take the log of the variable (This lowers the leverage of large values -- why?)

- Do nothing. Outliers are not always bad. Some people are "far" from the average. It may not make sense to try to change this variation.

---
# Missing data

Similarly, missing data can affect your results.

.mono[R] doesn't know how to deal with a missing observation.
```{R, examplena}
1 + 2 + 3 + NA + 5
```

If you run a regression<sup>â€ </sup> with missing values, .mono[R] drops the observations missing those values.

If the observations are missing in a nonrandom way, a random sample may end up nonrandom.

.footnote[
[â€ ]: Or perform almost any operation/function
]

---

layout: false
class: clear, middle, inverse
# Measures of model fit

---
# Measures of model fit

### Goal: quantify how "well" your regression model fits the data

#### General idea: Larger variance in residuals suggests our model isn't very predictive

.pull-left[

```{R, highvar, echo = F, dev = "svg"}
x = 1:100
yh = x + 40*rnorm(100)
yl = x + 5*rnorm(100)
df = tibble(x=x, yh=yh, yl=yl)

ggplot(data = df, aes(x, yh)) +
geom_point(color="darkslategrey", size=6) +
  geom_smooth(method='lm', formula= y~x, color="lightcoral", se=F, size=3) + 
theme_empty
```

]

--

.pull-right[

```{R, lowvar, echo = F, dev = "svg"}
ggplot(data = df, aes(x, yl)) +
geom_point(color="darkslategrey", size=6) +
  geom_smooth(method='lm', formula= y~x, color="lightcoral", se=F, size=3) +
theme_empty
```

]
---
# Coefficient of determination

- We already learned one measure of the strength of a linear relationship: correlation, $r$

--

- In OLS, we often rely on $R^2$, the **coefficient of determination**. In simple linear regression, this is simply the square of the correlation.
- Interpretation of $R^2$: **share of the variance in $y$ that is explained by your regression model**

--
$$
SSR = \text{sum of squared residuals} = \sum_i (y_i - \hat y_i)^2 = \sum_i e_i^2 
$$

$$
SST = \text{total sum of squares} = \sum_i (y_i - \bar y)^2 
$$

$$
R^2 = 1 - \frac{SSR}{SST} = 1 - \frac{\sum_i e_i^2}{\sum_i (y_i - \bar y)^2}
$$


---
# Coefficient of determination

$$
R^2 = 1 - \frac{SSR}{SST} = 1 - \frac{\sum_i e_i^2}{\sum_i (y_i - \bar y)^2}
$$
--

- $R^2$ varies between 0 and 1: Perfect model with $e_i=0$ for all $i$ has $R^2=1$. $R^2=0$ if we just guess the mean $\bar y$.

--

- In more complex models, $R^2$ is not the same as the square of the correlation coefficient. You should think of them as related but distinct concepts.

---
# Coefficient of determination

About 49% of the variation in ozone can be explained with temperature alone!

```{r, out.width = "70%", echo=FALSE, fig.align='center'}
knitr::include_graphics("ozone_temp_r2.png")
```

---
# Coefficient of determination

#### Definition: % of variance in $y$ that is explained by $x$ (and any other independent variables)

--

- Describes a _linear_ relationship between $y$ and $\hat y$
 
--

- Higher $R^2$ does not mean a model is "better" or more appropriate
  + Predictive power is not often the goal of regression analysis (e.g., you may just care about getting $\beta_1$ right)
  + If you are focused on predictive power, many other measures of fit can be appropriate (to discuss in machine learning)
  + Always look at your data and residuals! 
  
--

- Like OLS in general, $R^2$ is very sensitive to outliers. Again...always look at your data!

---
# Coefficient of determination 

Here, $R^2=0.94$ for a model of $y = \beta_0 + \beta_1 x + \epsilon$. Does that mean a linear relationship with $x$ is appropriate?


```{R, rsq, echo = F, dev = "svg", fig.height=5}
set.seed(444)
x = 1:100
y = x^2 + 100*rnorm(100)
dfr2 = tibble(x=x, y=y)

ggplot(data = dfr2, aes(x, y)) +
geom_point(color="darkslategrey", size=6) +
  geom_smooth(method='lm', formula= y~x, color="lightcoral", se=F, size=3) + 
theme_empty
```

---
# Coefficient of determination 

Here, $R^2=0$ for a model of $y = \beta_0 + \beta_1 x + \epsilon$. Does that mean there is no relationship between these variables?


```{R, rsq2, echo = F, dev = "svg", fig.height=5}
set.seed(444)
x = -50:50
y = x^2 + 100*rnorm(100)
dfr2 = tibble(x=x, y=y)

ggplot(data = dfr2, aes(x, y)) +
geom_point(color="darkslategrey", size=6) +
  geom_smooth(method='lm', formula= y~x, color="lightcoral", se=F, size=3) + 
theme_empty
```

---

layout: false
class: clear, middle, inverse
# Indicator/categorical variables

---
# Categorical variables

We have been talking a lot about **numerical** variables in linear regression...
  + Ozone levels
  + Crab size
  + Temperature and precipitation amounts
  + etc.

--

...but a lot of variables of interest are **categorical**:
  + Male/female
  + Presence/absence of a species
  + In/out of compliance with a pollution standard
  + etc.

--

#### How do we execute and interpret linear regression with categorical data?

---
# Categorical variables

We use **dummy** or **indicator** variables in linear regression to capture the influence of a categorical independent variable (_x_) on a continuous dependent variable (_y_).

--

For example, let _x_ be a categorical variable indicating the gender of an individual. Suppose we are interested in the "gender wage gap", so _y_ is income We estimate:

$$y_i = \beta_0 + \beta_1 MALE_i + \varepsilon_i $$
--

###Interpretation [draw it]:
- $MALE_i$ is an **indicator** variable that = 1 when $i$ is male (0 otherwise)
- $\beta_0=$ average wages if $i$ is **not** male
- $\beta_0+\beta_1=$ average wages if $i$ is male
- $\beta_1=$ average _difference_ in wages between males and females

---
# Categorical variables

#### For a categorical variable with two "levels", the OLS slope coefficient is the _difference_ in means across the two groups

```{r, echo=FALSE, fig.height=5.5}
set.seed(444)
male = sample(c(0,1), 100, replace=TRUE) # draw gender
income = male*40000 + rnorm(100, mean = 20000, sd= 20000) # draw income
df = as.data.frame(cbind(male,income)) 
df = df %>% mutate(male = factor(male, levels = c(0,1))) # make gender a factor
df = df %>% rowwise() %>% mutate(income = max(income,0)) # ensure income is nonnegative

ggplot(data=df) +
  geom_point(aes(y=income,x=male, color=male), size=4) + 
  scale_color_discrete() + 
  theme_classic()
  
```

---
# Categorical variables

#### What if I have many categories?
- E.g., species, education level, age group, ...

For example, let _x_ be a categorical variable indicating the species of penguin, and _y_ is body mass. We estimate:

$$y_i = \beta_0 + \beta_1 SPECIES_i + \varepsilon_i $$
Where **species** can be one of:
- Adelie
- Chinstrap
- Gentoo

---
# Categorical variables

```{r, echo=T}
library(palmerpenguins) 
head(penguins)
class(penguins$species)
```

---
# Categorical variables

```{r, echo=T}
summary(lm(body_mass_g ~ species, data = penguins))
```

---
# Categorical variables

What is going on here?? One _x_ variable turned into multiple slope coefficients? ðŸ¤”

--

.mono[R] is turning our regression

$$y_i = \beta_0 + \beta_1 SPECIES_i + \varepsilon_i $$

where _SPECIES_ is a categorical variable indicating one of three species, into:

$$y_i = \beta_0 + \beta_1 CHINSTRAP_i +\beta_2 GENTOO_i + \varepsilon_i $$

where _CHINSTRAP_ and _GENTOO_ are dummy variables for the Chinstrap and Gentoo species, respectively.

---
# Categorical variables

When your categorical variable takes on $k$ values, .mono[R] will create dummy variables for $k-1$ values, leaving one as the **reference** group:

```{r, echo=F}
knitr::include_graphics("penguins.png")
```

--

To evaluate the outcome for the reference group, **set the dummy variables equal to zero for all other groups**.

> Q: What is the average body mass of an Adelie species?

> Q: What is the difference in body mass between Chinstrap and Adelie?


---
layout: false
class: clear, middle, inverse
# Multiple linear regression

---

# More explanatory variables

We're moving from **simple linear regression** (one .pink[outcome variable] and one .purple[explanatory variable])

$$ \color{#e64173}{y_i} = \beta_0 + \beta_1 \color{#6A5ACD}{x_i} + u_i $$

--

to the land of **multiple linear regression** (one .pink[outcome variable] and multiple .purple[explanatory variables])

$$ \color{#e64173}{y\_i} = \beta\_0 + \beta\_1 \color{#6A5ACD}{x\_{1i}} + \beta\_2 \color{#6A5ACD}{x\_{2i}} + \cdots + \beta\_k \color{#6A5ACD}{x\_{ki}} + u\_i $$

--

**Why?**
--
 We can better explain the variation in $y$, improve predictions, avoid omitted-variable bias (i.e., second assumption needed for unbiased OLS estimates), ...

---
# More explanatory variables

Multiple linear regression...

$$ \color{#e64173}{y\_i} = \beta\_0 + \beta\_1 \color{#6A5ACD}{x\_{1i}} + \beta\_2 \color{#6A5ACD}{x\_{2i}} + \cdots + \beta\_k \color{#6A5ACD}{x\_{ki}} + u\_i $$
--

... raises many questions:

--

- Which $x$'s should I include? This is the problem of "model selection".

--

- How does my interpretation of $\beta_1$ change?

--

- What if my $x$'s interact with each other? E.g., race and gender, temperature and rainfall.

--

- How do I measure model fit now?

--

**We will dig into each of these here,** and you will see these questions in other MEDS courses

---
# Multiple regression

```{R, gendata, cache = T, include = F}
n <- 1e2
set.seed(1234)
gen_df <- tibble(
  x1 = runif(n = n, min = -3, max = 3),
  x2 = sample(x = c(F, T), size = n, replace = T),
  u  = rnorm(n = n, mean = 0, sd = 1),
  y  = -0.5 + x1 + x2 * 4 + u
)
mean_a <- filter(gen_df, x2 == F)$y %>% mean()
mean_b <- filter(gen_df, x2 == T)$y %>% mean()
gen_df %<>% mutate(y_dm = y - mean_a * (x2 == F) - mean_b * (x2 == T))
```

$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i \quad$ $x_1$ is continuous $\quad x_2$ is categorical

--

```{R, multregplot1, dev = "svg", echo = F, fig.height = 6}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 8) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 8) +
geom_point(size = 3) +
ylim(c(-4.5, 8)) +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
  theme_bw() + theme(
    line = element_blank(),
    rect = element_blank(),
    strip.text = element_blank(),
    axis.text = element_blank(),
    plot.title = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom",
    text = element_text(size = 20))
```



---
# Multiple regression

The intercept and categorical variable $x_2$ control for the groups' means.

```{R, multregplot2, dev = "svg", echo = F, fig.height =6}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
geom_hline(yintercept = mean_a, color = "darkslategrey", alpha = 0.5) +
geom_hline(yintercept = mean_b, color = red_pink, alpha = 0.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 8) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 8) +
geom_point(size = 3) +
ylim(c(-4.5, 8)) +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
  theme_bw() + theme(
    line = element_blank(),
    rect = element_blank(),
    strip.text = element_blank(),
    axis.text = element_blank(),
    plot.title = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom",
    text = element_text(size = 20))
```

---
# Multiple regression


$\hat{\beta}_1$ estimates the relationship between $y$ and $x_1$ after controlling for $x_2$. This is often called the "parallel slopes" model (one slope $\beta_1$ for each of the groups in $x_2$)

```{R, multregplot5, dev = "svg", echo = F, fig.height = 5.5}
ggplot(data = gen_df, aes(y = y, x = x1, color = x2, shape = x2)) +
geom_smooth(method = lm, se = F) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
annotate("text", x = -0.075, y = 7.75, label = TeX("$y$"), size = 8) +
annotate("text", x = 2.95, y = 0.3, label = TeX("$x_1$"), size = 8) +
geom_point(size = 3) +
ylim(c(-4.5, 8)) +
scale_color_manual(
  expression(x[2]),
  values = c("darkslategrey", red_pink),
  labels = c("A", "B")
) +
scale_shape_manual(
  expression(x[2]),
  values = c(1, 19),
  labels = c("A", "B")
) +
  theme_bw() + theme(
    line = element_blank(),
    rect = element_blank(),
    strip.text = element_blank(),
    axis.text = element_blank(),
    plot.title = element_blank(),
    axis.title = element_blank(),
    legend.position = "bottom",
    text = element_text(size = 20))
```


---
# Multiple regression

More generally, how do we think about multiple explanatory variables?

--

Suppose $y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i$

```{R, gendata2, cache = T, include = F}
n <- 1e2
set.seed(1234)
mr_df <- tibble(
  x1 = runif(n = n, min = -3, max = 3),
  x2 = runif(n=n, min = 5, max = 15),
  u  = rnorm(n = n, mean = 0, sd = 1),
  y  = -1 - 1*x1 + x2 * 4 + u
)
```

.pull-left[

```{R, yx1, echo = F, dev = "svg"}
ggplot(data = mr_df, aes(x=x1, y=y)) +
geom_point(color="darkslategrey", size=6) +
theme_bw() + theme(
    strip.text = element_blank(),
    axis.text = element_blank(),
    plot.title = element_blank(),
    text = element_text(size = 20))
```

]

--

.pull-right[

```{R, yx2, echo = F, dev = "svg"}
ggplot(data = mr_df, aes(x=x2, y=y)) +
geom_point(color="lightcoral", size=6) +
theme_bw() + theme(
    strip.text = element_blank(),
    axis.text = element_blank(),
    plot.title = element_blank(),
    text = element_text(size = 20))
```

]

---
# Multiple regression

More generally, how do we think about multiple explanatory variables?

```{R, 3dplot, echo = F, dev = "svg"}
library(plotly)

plot_ly(x=mr_df$x1, y=mr_df$x2, z=mr_df$y, type="scatter3d", mode="markers", color=mr_df$y) %>%
  layout(scene = list(xaxis = list(title = "x1"), yaxis = list(title = "x2"), zaxis = list(title = "y")))

```

---
# Multiple regression

### With **many** explanatory variables, we visualizing relationships means thinking about **hyperplanes** ðŸ¤¯

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i$$
#### Math notation looks very similar to simple linear regression, but _conceptually_ and _visually_ multiple regression is **very different**

---
# Multiple regression

### Interpretation of coefficients

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_k x_{ki} + u_i$$

--

- $\beta_k$ tells us the change in $y$ due to a one unit change in $x_k$ when **all other variables are held constant**

--

- This is an "all else equal" interpretation

--

- E.g., how much do wages increase with one more year of education, _holding gender fixed_? 

--

- E.g., how much does ozone increase when temperature rises, _holding NOx emissions fixed_? 

---
# Tradeoffs

There are tradeoffs to consider as we add/remove variables:

**Fewer variables**

- Generally explain less variation in $y$
- Provide simple interpretations and visualizations (*parsimonious*)
- May need to worry about omitted-variable bias

**More variables**

- More likely to find *spurious* relationships (statistically significant due to chanceâ€”does not reflect a true, population-level relationship)
- More difficult to interpret the model
- You may still miss important variablesâ€”still omitted-variable bias

---
# Omitted-variable bias

You will study this in much more depth in EDS 241, but here's a primer.

**Omitted-variable bias** (OVB) arises when we omit a variable that

1. affects our outcome variable $y$

2. correlates with an explanatory variable $x_j$

As it's name suggests, this situation leads to bias in our estimate of $\beta_j$. In particular, it violates Assumption 2 of OLS from last week. 

--

**Note:** OVB Is not exclusive to multiple linear regression, but it does require multiple variables affect $y$.

---
# Omitted-variable bias

**Example**

Let's imagine a simple model for the cancer rates in census tract $i$:

$$ \text{Cancer rate}_i = \beta_0 + \beta_1 \text{UV radiation}_i + \beta_2 \text{TRI}_i + u_i $$

where

- $\text{UV radiation}_i$ gives the average UV radiation in tract $i$ (mW/cm$^2$) 
- $\text{TRI}_i$ denotes an indicator variable for whether tract $i$ has a Toxics Release Inventory facility 

thus

- $\beta_1$: the change in cancer rate associated with a 1 mW/cm$^2$ increase in UV radiation (*ceteris paribus*)
- $\beta_2$: the difference in avg. cancer rates between TRI and non-TRI census tracts (*ceteris paribus*)
<br>If $\beta_2 > 0$, then TRI tracts have higher cancer rates

---

# Omitted-variable bias


```{R, genovbdata, include = F, cache = T}
# Set seed
set.seed(12345)
# Sample size
n <- 1e3
# Parameters
beta0 <- 20; beta1 <- 0.5; beta2 <- 10
# Dataset
omit_df <- tibble(
  TRI = sample(x = c(F, T), size = n, replace = T),
  uv = runif(n, 3, 9) - 3 * TRI,
  cancer = beta0 + beta1 * uv + beta2 * TRI + rnorm(n, sd = 7)
)
lm_bias <- lm(cancer ~ uv, data = omit_df)
bb0 <- lm_bias$coefficients[1] %>% round(1)
bb1 <- lm_bias$coefficients[2] %>% round(1)
lm_unbias <- lm(cancer ~ uv + TRI, data = omit_df)
bu0 <- lm_unbias$coefficients[1] %>% round(1)
bu1 <- lm_unbias$coefficients[2] %>% round(1)
bu2 <- lm_unbias$coefficients[3] %>% round(1)
```


**"True" relationship:** $\text{Cancer rate}_i = `r beta0` + `r beta1` \times \text{UV radiation}_i + `r beta2` \times \text{TRI}_i + u_i$

The relationship between cancer rates and UV radiations:

```{R, plotovb1, echo = F, dev = "svg", fig.height = 5}
ggplot(data = omit_df, aes(x = uv, y = cancer)) +
geom_point(size = 2.5, color = "black", alpha = 0.4, shape = 16) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
xlab("UV radiation") +
ylab("Cancer rate") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```
---
# Omitted-variable bias

Biased regression estimate: $\widehat{\text{Cancer rate}}_i = `r bb0` + `r bb1` \times \text{UV radiation}_i$

```{R, plotovb2, echo = F, dev = "svg", fig.height = 5}
ggplot(data = omit_df, aes(x = uv, y = cancer)) +
geom_point(size = 2.5, color = "black", alpha = 0.4, shape = 16) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_smooth(se = F, color = "orange", method = lm) +
xlab("UV radiation") +
ylab("Cancer rate") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"))
```

---
# Omitted-variable bias

Recalling the omitted variable: TRI (**<font color="#e64173">non-TRI</font>** and **<font color="#314f4f">TRI</font>**)

```{R, plotovb3, echo = F, dev = "svg", fig.height = 5}
ggplot(data = omit_df, aes(x = uv, y = cancer)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = TRI, shape = TRI)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.5, size = 1) +
xlab("UV radiation") +
ylab("Cancer rate") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("non-TRI", "TRI")) +
scale_shape_manual("", values = c(16, 1), labels = c("non-TRI", "TRI"))
```

---
# Omitted-variable bias

Recalling the omitted variable: TRI (**<font color="#e64173">non-TRI</font>** and **<font color="#314f4f">TRI</font>**)

```{R, plotovb4, echo = F, dev = "svg", fig.height = 5}
ggplot(data = omit_df, aes(x = uv, y = cancer)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = TRI, shape = TRI)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.2, size = 1) +
geom_abline(
  intercept = lm_unbias$coefficients[1],
  slope = lm_unbias$coefficients[2],
  color = red_pink, size = 1
) +
geom_abline(
  intercept = lm_unbias$coefficients[1] + lm_unbias$coefficients[3],
  slope = lm_unbias$coefficients[2],
  color = "darkslategrey", size = 1
) +
xlab("UV radiation") +
ylab("Cancer rate") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("non-TRI", "TRI")) +
scale_shape_manual("", values = c(16, 1), labels = c("non-TRI", "TRI"))
```

---
# Omitted-variable bias

Unbiased regression estimate: $\widehat{\text{Cancer rate}}_i = `r bu0` + `r bu1` \times \text{UV radiation}_i + `r bu2` \times \text{TRI}_i$

```{R, plotovb5, echo = F, dev = "svg", fig.height = 5}
ggplot(data = omit_df, aes(x = uv, y = cancer)) +
geom_point(size = 2.5, alpha = 0.8, aes(color = TRI, shape = TRI)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_line(stat = "smooth", color = "orange", method = lm, alpha = 0.2, size = 1) +
geom_abline(
  intercept = lm_unbias$coefficients[1],
  slope = lm_unbias$coefficients[2],
  color = red_pink, size = 1
) +
geom_abline(
  intercept = lm_unbias$coefficients[1] + lm_unbias$coefficients[3],
  slope = lm_unbias$coefficients[2],
  color = "darkslategrey", size = 1
) +
xlab("UV radiation") +
ylab("Cancer rate") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("non-TRI", "TRI")) +
scale_shape_manual("", values = c(16, 1), labels = c("non-TRI", "TRI"))
```

---

class: center, middle


Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

Some slide components come from [Ed Rubin's](https://github.com/edrubin/EC421S20) awesome course materials.


```{css, echo = FALSE}
@media print {
  .has-continuation {
    display: block;
  }
}
```
  
---
exclude: true


```{R, print pdfs, echo = F, eval = F}
pagedown::chrome_print(
  input = "04-ols-contd.html",
  output = "04-ols-contd.pdf",
  timeout = 60
)
```

